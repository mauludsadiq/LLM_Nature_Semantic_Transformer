import os
import sys
import json
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

torch.set_num_threads(1)
try:
    torch.set_num_interop_threads(1)
except Exception:
    pass

def emit(obj, exit_code=0):
    sys.stdout.write(json.dumps(obj, ensure_ascii=False) + "\n")
    sys.exit(exit_code)

query = sys.argv[1] if len(sys.argv) > 1 else ""

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)

if not torch.backends.mps.is_available():
    emit({
        "ok": False,
        "error": "MPS not available (CPU forward crashes with Bus error on this machine)",
        "query": query,
        "prompt": None,
        "raw_text": "",
        "ops": [],
        "fallback_used": True,
        "meta": {"device": "cpu", "inference_s": None, "tokens_generated": 0},
    }, exit_code=2)

device = torch.device("mps")
model = AutoModelForCausalLM.from_pretrained(model_name)
model = model.to(device)
model.eval()

OPS_V0 = [
    "LOAD 7/200",
    "MASK_BIT bit=2 val=1",
    "WITNESS_NEAREST target=7/200",
    "RETURN_SET",
]

CANDIDATES = [
    "\n".join(OPS_V0) + "\n",
]

prompt = (
    "You are a semantic trace generator.\n"
    "Convert the query into a sequence of operations.\n"
    "Output ONLY the trace, no commentary.\n\n"
    f"Query: {query}\n"
    "Trace:\n"
)

try:
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    prompt_len = int(inputs["input_ids"].shape[1])

    # Warm up MPS allocations (avoids placeholder storage errors on some ops)
    with torch.no_grad():
        _ = model(**inputs)

    class TrieNode:
        __slots__ = ("children", "terminal")
        def __init__(self):
            self.children = {}
            self.terminal = False

    def build_trie(seqs_token_ids):
        root = TrieNode()
        max_len = 0
        for seq in seqs_token_ids:
            max_len = max(max_len, len(seq))
            cur = root
            for tid in seq:
                nxt = cur.children.get(tid)
                if nxt is None:
                    nxt = TrieNode()
                    cur.children[tid] = nxt
                cur = nxt
            cur.terminal = True
        return root, max_len

    seqs = [tokenizer.encode(s, add_special_tokens=False) for s in CANDIDATES]
    trie_root, max_new = build_trie(seqs)

    def allowed_tokens(batch_id, input_ids):
        # transformers may pass input_ids as 1D (seq_len,) or 2D (1, seq_len)
        if hasattr(input_ids, "dim") and int(input_ids.dim()) == 1:
            full = input_ids.tolist()
            seq_len = int(input_ids.shape[0])
        else:
            full = input_ids[0].tolist()
            seq_len = int(input_ids.shape[-1])

        gen_len = seq_len - prompt_len
        if gen_len < 0:
            gen_len = 0

        gen_ids = full[prompt_len:prompt_len + gen_len]
        cur = trie_root
        for tid in gen_ids:
            nxt = cur.children.get(int(tid))
            if nxt is None:
                eos = tokenizer.eos_token_id
                return [int(eos)] if eos is not None else [0]
            cur = nxt

        if cur.children:
            return [int(t) for t in cur.children.keys()]

        eos = tokenizer.eos_token_id
        return [int(eos)] if eos is not None else [0]

    t0 = time.time()
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new,
            do_sample=False,
            num_beams=1,
            prefix_allowed_tokens_fn=allowed_tokens,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    t1 = time.time()

    gen_ids = out[0][prompt_len:]
    raw_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    tokens_generated = int(gen_ids.numel()) if hasattr(gen_ids, "numel") else int(len(gen_ids))

    # Strict validation against the grammar language (NO fallback on invalid).
    if raw_text not in CANDIDATES:
        emit({
            "ok": False,
            "error": "generated output not in trace grammar language",
            "query": query,
            "prompt": prompt,
            "raw_text": raw_text,
            "ops": [],
            "fallback_used": True,
            "meta": {
                "device": "mps",
                "inference_s": float(t1 - t0),
                "tokens_generated": int(tokens_generated),
            },
        }, exit_code=2)

    ops = [ln.strip() for ln in raw_text.splitlines() if ln.strip()]

    emit({
        "ok": True,
        "query": query,
        "prompt": prompt,
        "raw_text": raw_text,
        "ops": ops,
        "fallback_used": False,
        "meta": {
            "device": "mps",
            "inference_s": float(t1 - t0),
            "tokens_generated": int(tokens_generated),
        },
    }, exit_code=0)

except Exception as e:
    emit({
        "ok": False,
        "error": f"exception: {type(e).__name__}: {e}",
        "query": query,
        "prompt": prompt,
        "raw_text": "",
        "ops": [],
        "fallback_used": True,
        "meta": {"device": "mps", "inference_s": None, "tokens_generated": 0},
    }, exit_code=2)
